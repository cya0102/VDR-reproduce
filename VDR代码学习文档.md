# VDR (Vocabulary Disentangled Retrieval) ä»£ç å­¦ä¹ æ–‡æ¡£

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ ¸å¿ƒæ€æƒ³](#2-æ ¸å¿ƒæ€æƒ³)
3. [ä»£ç æ¶æ„](#3-ä»£ç æ¶æ„)
4. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#4-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
5. [è®­ç»ƒæµç¨‹](#5-è®­ç»ƒæµç¨‹)
6. [æ¨ç†æµç¨‹](#6-æ¨ç†æµç¨‹)
7. [å…³é”®åˆ›æ–°ç‚¹](#7-å…³é”®åˆ›æ–°ç‚¹)
8. [ä½¿ç”¨ç¤ºä¾‹](#8-ä½¿ç”¨ç¤ºä¾‹)
9. [è®ºæ–‡è¦ç‚¹](#9-è®ºæ–‡è¦ç‚¹)

---

## 1. é¡¹ç›®æ¦‚è¿°

VDRï¼ˆVocabulary Disentangled Retrievalï¼‰æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼Œå‘è¡¨äºICLR 2024ã€‚å®ƒå°†æ–‡æœ¬æ˜ å°„åˆ°è¯æ±‡ç©ºé—´çš„ç¨€ç–è¡¨ç¤ºï¼Œå®ç°äº†**å¯è§£é‡Š**å’Œ**é«˜æ•ˆ**çš„æ£€ç´¢ã€‚

### ä¸»è¦ç‰¹ç‚¹

- âœ… **å¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªç»´åº¦å¯¹åº”è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªtokenï¼Œå¯ä»¥ç›´è§‚ç†è§£æ¨¡å‹å…³æ³¨ä»€ä¹ˆ
- âœ… **é«˜æ•ˆæ£€ç´¢**ï¼šä½¿ç”¨ç¨€ç–å‘é‡ï¼Œæ”¯æŒå¿«é€Ÿçš„å€’æ’ç´¢å¼•æ£€ç´¢
- âœ… **åŠå‚æ•°åŒ–**ï¼šç»“åˆè¯æ±‡åŒ¹é…ï¼ˆBOWï¼‰å’Œè¯­ä¹‰ç†è§£ï¼ˆç¥ç»ç½‘ç»œï¼‰
- âœ… **è·¨æ¨¡æ€æ”¯æŒ**ï¼šå¯æ‰©å±•åˆ°æ–‡æœ¬-å›¾åƒæ£€ç´¢

### ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”

| æ–¹æ³• | è¡¨ç¤ºç»´åº¦ | å¯è§£é‡Šæ€§ | æ£€ç´¢é€Ÿåº¦ | è¯­ä¹‰ç†è§£ |
|------|---------|---------|---------|---------|
| **BM25** | è¯æ±‡ç©ºé—´ | âœ… é«˜ | âœ… å¿« | âŒ æ—  |
| **DPR** | 768ç»´ç¨ å¯† | âŒ æ—  | âš ï¸ ä¸­ç­‰ | âœ… å¼º |
| **VDR** | 30Kç»´ç¨€ç– | âœ… é«˜ | âœ… å¿« | âœ… å¼º |

---

## 2. æ ¸å¿ƒæ€æƒ³

### 2.1 è¡¨ç¤ºç©ºé—´

VDRå°†æ–‡æœ¬æ˜ å°„åˆ°**è¯æ±‡ç©ºé—´**çš„å‘é‡ï¼š

```
è¾“å…¥: "Einstein developed the theory of relativity"
     â†“ BERTç¼–ç 
ä¸Šä¸‹æ–‡è¡¨ç¤º: [batch, seq_len, 768]
     â†“ æŠ•å½±åˆ°è¯æ±‡ç©ºé—´
è¯æ±‡å‘é‡: [batch, 30522] (BERTè¯æ±‡è¡¨å¤§å°)
     â†“ Top-Kç¨€ç–åŒ–
ç¨€ç–å‘é‡: [batch, 30522] (ä»…768ä¸ªéé›¶å…ƒç´ )

ç»“æœç¤ºä¾‹:
{
  "einstein": 15.3,    â† è¾“å…¥ä¸­å‡ºç°
  "relativity": 12.8,  â† è¾“å…¥ä¸­å‡ºç°
  "theory": 10.5,      â† è¾“å…¥ä¸­å‡ºç°
  "physics": 8.2,      â† è¯­ä¹‰ç›¸å…³ï¼Œæœªå‡ºç°
  "scientist": 7.1,    â† è¯­ä¹‰ç›¸å…³ï¼Œæœªå‡ºç°
  ...
}
```

### 2.2 æ ¸å¿ƒæœºåˆ¶

#### æŠ•å½±åˆ°è¯æ±‡ç©ºé—´

```python
# å…³é”®æ“ä½œï¼ˆåœ¨forwardæ–¹æ³•ä¸­ï¼‰
last_hidden_state = bert_model(input_ids)  # [batch, seq_len, 768]
ln_output = layer_norm(last_hidden_state)   # å½’ä¸€åŒ–

# ä½¿ç”¨BERTçš„è¯åµŒå…¥çŸ©é˜µä½œä¸º"ç æœ¬"
word_embeddings = bert_model.embeddings.word_embeddings.weight  # [30522, 768]
vocab_scores = ln_output @ word_embeddings[999:].T  # [batch, seq_len, 29523]

# æ¿€æ´»å‡½æ•°ï¼ˆç¡®ä¿éè´Ÿï¼‰
vocab_scores = ELU(vocab_scores) + 1

# è·¨åºåˆ—æ± åŒ–
vocab_emb = max_pooling(vocab_scores, dim=1)  # [batch, 29523]
```

**å…³é”®æ€æƒ³**ï¼šå°†BERTçš„è¯åµŒå…¥ç©ºé—´ä½œä¸º"ç æœ¬"ï¼Œè®¡ç®—æ¯ä¸ªä½ç½®ä¸æ‰€æœ‰è¯æ±‡çš„ç›¸ä¼¼åº¦ã€‚

#### ç¨€ç–åŒ–ç­–ç•¥

```python
# Top-Kç¨€ç–åŒ–ï¼ˆä¿ç•™æœ€é‡è¦çš„768ä¸ªç»´åº¦ï¼‰
topk_mask = build_topk_mask(vocab_emb, k=768)
vocab_emb_sparse = vocab_emb * topk_mask

# BOW maskï¼ˆæ ‡è®°è¾“å…¥ä¸­å‡ºç°çš„è¯ï¼‰
bow_mask = build_bow_mask(input_ids)

# ç»„åˆï¼šç¡®ä¿è¾“å…¥è¯ä¸€å®šè¢«æ¿€æ´»
final_mask = topk_mask | bow_mask
output = vocab_emb * final_mask
```

### 2.3 åŠå‚æ•°åŒ–æ£€ç´¢

VDRç»“åˆä¸¤ç§è¡¨ç¤ºï¼š

1. **å‚æ•°åŒ–è¡¨ç¤º**ï¼ˆè¯­ä¹‰ï¼‰ï¼šé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ çš„top-Kç»´åº¦
2. **éå‚æ•°åŒ–è¡¨ç¤º**ï¼ˆè¯æ±‡ï¼‰ï¼šç²¾ç¡®çš„BOWåŒ¹é…

```
ç›¸ä¼¼åº¦è®¡ç®—:
score(Q, P) = âŸ¨Q_topk, P_denseâŸ© + Î»Â·âŸ¨Q_bow, P_bowâŸ©
              â†‘ è¯­ä¹‰åŒ¹é…           â†‘ è¯æ±‡åŒ¹é…
```

---

## 3. ä»£ç æ¶æ„

### ç›®å½•ç»“æ„

```
VDR-reproduce/
â”œâ”€â”€ src/ir/
â”‚   â”œâ”€â”€ encoder/           # ç¼–ç å™¨å®ç°
â”‚   â”‚   â”œâ”€â”€ vdr.py        # â­ VDRç¼–ç å™¨ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”‚   â”œâ”€â”€ dpr.py        # DPRç¼–ç å™¨ï¼ˆå¯¹æ¯”åŸºçº¿ï¼‰
â”‚   â”‚   â””â”€â”€ types.py      # ç¼–ç å™¨ç±»å‹æ³¨å†Œ
â”‚   â”œâ”€â”€ biencoder/        # åŒç¼–ç å™¨æ¶æ„
â”‚   â”‚   â””â”€â”€ biencoder.py  # BiEncoderå®ç°
â”‚   â”œâ”€â”€ retriever/        # æ£€ç´¢å™¨
â”‚   â”‚   â””â”€â”€ retriever.py  # Retrieverç±»ï¼ˆæ£€ç´¢æ¥å£ï¼‰
â”‚   â”œâ”€â”€ index/            # ç´¢å¼•ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ base.py       # ç´¢å¼•åŸºç±»
â”‚   â”‚   â””â”€â”€ binary_token_index.py  # äºŒå€¼tokenç´¢å¼•
â”‚   â”œâ”€â”€ training/         # è®­ç»ƒå·¥å…·
â”‚   â”‚   â”œâ”€â”€ loss_utils.py # â­ æŸå¤±å‡½æ•°ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”‚   â”œâ”€â”€ ddp_utils.py  # åˆ†å¸ƒå¼è®­ç»ƒ
â”‚   â”‚   â””â”€â”€ model_utils.py # æ¨¡å‹å·¥å…·
â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ sparsify_utils.py  # â­ ç¨€ç–åŒ–å·¥å…·
â”‚       â””â”€â”€ visualize_utils.py # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ train_ir.py           # â­ è®­ç»ƒè„šæœ¬
â”œâ”€â”€ conf/                 # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ biencoder/        # ç¼–ç å™¨é…ç½®
â”‚   â”œâ”€â”€ train/            # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ data_stores/      # æ•°æ®é›†é…ç½®
â””â”€â”€ examples/             # ç¤ºä¾‹ä»£ç 
```

### æ ¸å¿ƒç±»å…³ç³»å›¾

```
PreTrainedModel (HuggingFace)
    â†‘
    â”œâ”€â”€ VDREncoder          # VDRç¼–ç å™¨
    â”œâ”€â”€ DPREncoder          # DPRç¼–ç å™¨
    â””â”€â”€ BiEncoder           # åŒç¼–ç å™¨
            â†‘
            â””â”€â”€ Retriever   # æ£€ç´¢å™¨ï¼ˆæ·»åŠ ç´¢å¼•åŠŸèƒ½ï¼‰
```

---

## 4. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 4.1 VDRç¼–ç å™¨ (`src/ir/encoder/vdr.py`)

#### VDREncoderç±»

```python
class VDREncoder(PreTrainedModel):
    """
    VDRç¼–ç å™¨ï¼šå°†æ–‡æœ¬æ˜ å°„åˆ°è¯æ±‡ç©ºé—´çš„ç¨€ç–å‘é‡è¡¨ç¤º
    """
    
    def __init__(self, config):
        # æ ¸å¿ƒç»„ä»¶
        self.bert_model = AutoModel.from_pretrained('bert-base-uncased')
        self.ln = LayerNorm(768)  # å½’ä¸€åŒ–å±‚
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
```

**å…³é”®å‚æ•°**ï¼š
- `shift_vocab_num=999`: è·³è¿‡å‰999ä¸ªç‰¹æ®Štokenï¼ˆæ ‡ç‚¹ã€ç‰¹æ®Šç¬¦å·ç­‰ï¼‰
- `topk=768`: ä¿ç•™768ä¸ªæœ€é‡è¦çš„ç»´åº¦
- `pooling='max'`: ä½¿ç”¨max poolingè·¨åºåˆ—èšåˆ

#### forwardæ–¹æ³•

```python
def forward(self, input_ids, token_type_ids, attention_mask):
    """
    å‰å‘ä¼ æ’­ï¼šæ–‡æœ¬ â†’ è¯æ±‡ç©ºé—´è¡¨ç¤º
    
    æµç¨‹:
    1. BERTç¼–ç  â†’ [batch, seq_len, 768]
    2. Layer Norm
    3. æŠ•å½±åˆ°è¯æ±‡ç©ºé—´ â†’ [batch, seq_len, vocab_size]
    4. ELU+1æ¿€æ´»ï¼ˆç¡®ä¿éè´Ÿï¼‰
    5. Max pooling â†’ [batch, vocab_size]
    6. å¯é€‰: L2å½’ä¸€åŒ–
    """
    # Step 1: BERTç¼–ç 
    outputs = self.bert_model(input_ids, token_type_ids, attention_mask)
    hidden = outputs.last_hidden_state  # [batch, seq_len, 768]
    
    # Step 2: å½’ä¸€åŒ–
    hidden = self.ln(hidden)
    
    # Step 3: æŠ•å½±åˆ°è¯æ±‡ç©ºé—´
    # ä½¿ç”¨è¯åµŒå…¥çŸ©é˜µçš„è½¬ç½®ä½œä¸ºæŠ•å½±çŸ©é˜µ
    word_emb = self.bert_model.embeddings.word_embeddings.weight[999:]
    vocab_scores = hidden @ word_emb.T  # [batch, seq_len, vocab_size]
    
    # Step 4: æ¿€æ´»å‡½æ•°
    vocab_scores = elu1p(vocab_scores)  # ELU(x) + 1
    
    # Step 5: æ± åŒ–
    vocab_emb = vocab_scores.max(1)[0]  # [batch, vocab_size]
    
    return vocab_emb
```

#### embedæ–¹æ³•

```python
def embed(self, texts, topk=768, bow=False, activate_lexical=True):
    """
    æ¨ç†æ¥å£ï¼šå°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºç¨€ç–å‘é‡
    
    æ”¯æŒä¸‰ç§æ¨¡å¼:
    1. bow=True: çº¯BOWè¡¨ç¤ºï¼ˆä¸ä½¿ç”¨ç¥ç»ç½‘ç»œï¼‰
    2. topk=-1: ç¨ å¯†è¡¨ç¤ºï¼ˆä¿ç•™æ‰€æœ‰ç»´åº¦ï¼‰
    3. topk=K: ç¨€ç–è¡¨ç¤ºï¼ˆä¿ç•™top-Kç»´åº¦ï¼‰
    """
    for batch_texts in batches(texts):
        # 1. Tokenize
        encoding = self.tokenize(batch_texts)
        
        # 2. æ„å»ºBOW mask
        bow_mask = build_bow_mask(encoding.input_ids)
        
        if bow:
            # æ¨¡å¼1: çº¯BOW
            batch_emb = bow_mask
        else:
            # æ¨¡å¼2/3: ç¥ç»ç½‘ç»œç¼–ç 
            batch_emb = self.forward(**encoding)
            
            # æ„å»ºTop-K mask
            if topk > 0:
                topk_mask = build_topk_mask(batch_emb, k=topk)
            else:
                topk_mask = torch.ones_like(batch_emb)
            
            # ç»„åˆmask
            if activate_lexical:
                mask = topk_mask | bow_mask  # å¹¶é›†
            else:
                mask = topk_mask
            
            # åº”ç”¨mask
            batch_emb = batch_emb * mask
        
        yield batch_emb
```

#### disentangleæ–¹æ³•ï¼ˆå¯è§£é‡Šæ€§ï¼‰

```python
def disentangle(self, text, topk=768, visual=False):
    """
    è§£ç¼ æ–‡æœ¬ï¼šè¿”å›æœ€ç›¸å…³çš„è¯æ±‡åŠå…¶æƒé‡
    
    è¿™æ˜¯VDRå¯è§£é‡Šæ€§çš„æ ¸å¿ƒåŠŸèƒ½ï¼
    """
    # è·å–ç¨€ç–è¡¨ç¤º
    emb = self.embed(text)
    
    # è·å–top-Kçš„tokenåŠå…¶æƒé‡
    topk_result = emb.topk(topk)
    token_ids = topk_result.indices + 999  # è¿˜åŸåç§»
    weights = topk_result.values
    
    # è½¬æ¢ä¸ºå¯è¯»çš„token
    tokens = self.tokenizer.convert_ids_to_tokens(token_ids)
    
    # è¿”å› {token: weight} å­—å…¸
    return dict(zip(tokens, weights))
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
vdr = VDREncoder.from_pretrained("vsearch/vdr-nq")
result = vdr.disentangle("Who invented the theory of relativity?")

# è¾“å‡º:
# {
#   "einstein": 15.3,
#   "relativity": 12.8,
#   "theory": 10.5,
#   "physics": 8.2,
#   ...
# }
```

### 4.2 ç¨€ç–åŒ–å·¥å…· (`src/ir/utils/sparsify_utils.py`)

#### elu1pæ¿€æ´»å‡½æ•°

```python
elu1p = lambda x: F.elu(x) + 1

# ä¸ºä»€ä¹ˆç”¨ELU+1ï¼Ÿ
# 1. è¾“å‡ºéè´Ÿï¼ˆxâ‰¥-1åï¼ŒELU(x)+1â‰¥0ï¼‰
# 2. æ¯”ReLUæ›´å¹³æ»‘ï¼ˆx<0æ—¶æœ‰æ¢¯åº¦ï¼‰
# 3. æ¯”Softmaxæ›´é«˜æ•ˆï¼ˆä¸éœ€è¦æŒ‡æ•°è¿ç®—ï¼‰
```

#### build_topk_mask

```python
def build_topk_mask(embs, k=768):
    """
    æ„å»ºTop-Kæ©ç ï¼šä»…ä¿ç•™æœ€å¤§çš„Kä¸ªç»´åº¦
    
    è¿™æ˜¯VDRå®ç°ç¨€ç–åŒ–çš„æ ¸å¿ƒï¼
    """
    # 1. æ‰¾åˆ°top-kçš„å€¼å’Œç´¢å¼•
    values, indices = torch.topk(embs, k, dim=-1)
    
    # 2. åˆ›å»ºå…¨Falseçš„mask
    mask = torch.zeros_like(embs, dtype=torch.bool)
    
    # 3. å°†top-kä½ç½®è®¾ä¸ºTrue
    mask.scatter_(-1, indices, True)
    
    return mask

# ç¤ºä¾‹:
# embs = [[0.1, 0.5, 0.3, 0.9, 0.2]]
# mask = build_topk_mask(embs, k=2)
# ç»“æœ: [[False, True, False, True, False]]  # ä¿ç•™0.5å’Œ0.9
```

#### build_bow_mask

```python
def build_bow_mask(text_ids, vocab_size=30522, shift_num=999):
    """
    æ„å»ºè¯è¢‹æ©ç ï¼šæ ‡è®°è¾“å…¥ä¸­å‡ºç°è¿‡çš„token
    
    è¿™æ˜¯VDRå®ç°ç²¾ç¡®è¯æ±‡åŒ¹é…çš„å…³é”®ï¼
    """
    N, seq_len = text_ids.shape
    
    # åˆå§‹åŒ–å…¨é›¶çŸ©é˜µ
    bow = torch.zeros([N, vocab_size], device=text_ids.device)
    
    # ä½¿ç”¨scatter_å°†è¾“å…¥tokençš„ä½ç½®è®¾ä¸º1
    bow.scatter_(-1, text_ids, 1)
    
    # å»æ‰å‰shift_numä¸ªç‰¹æ®Štoken
    bow = bow[:, shift_num:]
    
    return bow.bool().float()

# ç¤ºä¾‹:
# text_ids = [[101, 2054, 2003]]  # [CLS] what is
# bow = build_bow_mask(text_ids)
# ç»“æœ: [[0, ..., 1, ..., 1, ...]]  # ä»…åœ¨2054å’Œ2003ä½ç½®ä¸º1
```

#### build_cts_maskï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰

```python
def build_cts_mask(bow_embs):
    """
    æ„å»ºå¯¹æ¯”æ©ç ï¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…"ä¸“å±"çš„è¯æ±‡å­é›†
    
    ç”¨é€”: åœ¨è®­ç»ƒæ—¶é¿å…è¯æ±‡å†²çªï¼Œå¢å¼ºè´Ÿé‡‡æ ·
    """
    batch_size, vocab_size = bow_embs.shape
    
    # 1. è®¡ç®—æ‰€æœ‰æ ·æœ¬ä¸­å‡ºç°è¿‡çš„è¯æ±‡ï¼ˆå¹¶é›†ï¼‰
    bow_batch = bow_embs.sum(0).bool()
    
    # 2. ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸“å±è¯æ±‡ï¼ˆä½¿ç”¨æ¨¡è¿ç®—ï¼‰
    vocab_indices = torch.arange(vocab_size)
    sample_indices = vocab_indices % batch_size
    cts_mask_init = (sample_indices.unsqueeze(0) == 
                     torch.arange(batch_size).unsqueeze(1))
    
    # 3. åªä¿ç•™æœªå‡ºç°è¿‡çš„è¯æ±‡
    cts_mask = cts_mask_init & ~bow_batch.unsqueeze(0)
    
    return cts_mask

# åŸç†: å°†30Kè¯æ±‡ç©ºé—´åˆ’åˆ†ç»™ä¸åŒæ ·æœ¬
# æ ·æœ¬0: [0, 4, 8, ...]  (vocab_idx % batch_size == 0)
# æ ·æœ¬1: [1, 5, 9, ...]  (vocab_idx % batch_size == 1)
# ...
```

### 4.3 æŸå¤±å‡½æ•° (`src/ir/training/loss_utils.py`)

#### compute_vdr_loss

VDRçš„è®­ç»ƒæŸå¤±æ˜¯å…¶æ ¸å¿ƒåˆ›æ–°ï¼Œå®ç°äº†**åŠå‚æ•°åŒ–å­¦ä¹ **ï¼š

```python
def compute_vdr_loss(cfg, q_emb, p_emb, q_bin, p_bin):
    """
    VDRåŠå‚æ•°åŒ–æŸå¤±å‡½æ•°
    
    ç»“åˆå››ä¸ªæŸå¤±é¡¹ï¼š
    1. L1: queryè¯­ä¹‰ Ã— passageç¨ å¯†
    2. L2: queryç¨ å¯† Ã— passageè¯­ä¹‰
    3. L3: queryè¯æ±‡ Ã— passageç¨ å¯†
    4. L4: queryç¨ å¯† Ã— passageè¯æ±‡
    
    æœ€ç»ˆæŸå¤± = (L1 + L2 + L3 + L4) / 4
    """
    N, V = q_emb.shape  # batch_size, vocab_size
    
    # Step 1: æ”¶é›†å…¨å±€å‘é‡ï¼ˆè·¨GPUï¼‰
    q_emb_global, q_topk_global, q_bow_global = fetch_global_vectors(q_emb, q_bin)
    p_emb_global, p_topk_global, p_bow_global = fetch_global_vectors(p_emb, p_bin)
    
    # Step 2: è®¡ç®—å››ä¸ªæŸå¤±
    # Loss 1: è¯­ä¹‰æ£€ç´¢ï¼ˆqueryä¾§ï¼‰
    loss_1 = contrastive_loss(q_topk_global, p_emb_global)
    
    # Loss 2: è¯­ä¹‰æ£€ç´¢ï¼ˆpassageä¾§ï¼‰
    loss_2 = contrastive_loss(q_emb_global, p_topk_global)
    
    # Loss 3: è¯æ±‡æ£€ç´¢ï¼ˆqueryä¾§ï¼‰
    loss_3 = contrastive_loss(q_bow_global, p_emb_global)
    
    # Loss 4: è¯æ±‡æ£€ç´¢ï¼ˆpassageä¾§ï¼‰
    loss_4 = contrastive_loss(q_emb_global, p_bow_global)
    
    # Step 3: ç»„åˆæŸå¤±
    loss = (loss_1 + loss_2 + loss_3 + loss_4) / 4
    
    return loss
```

**å…³é”®è®¾è®¡**ï¼š

1. **å¯¹ç§°æŸå¤±** (`sym_loss=True`)ï¼šåŒæ—¶ä¼˜åŒ–queryâ†’passageå’Œpassageâ†’query
2. **åŠå‚æ•°åŒ–** (`semi=True`)ï¼šåŒæ—¶è®­ç»ƒè¯­ä¹‰è¡¨ç¤ºå’Œè¯æ±‡è¡¨ç¤º
3. **å¯¹æ¯”æ©ç ** (`cts_mask=True`)ï¼šå¢å¼ºè´Ÿé‡‡æ ·ï¼Œé¿å…è¯æ±‡å†²çª

#### fetch_global_vectors

åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­æ”¶é›†æ‰€æœ‰GPUçš„å‘é‡ï¼š

```python
def fetch_global_vectors(emb_local, bow_local, k=768):
    """
    æ”¶é›†å…¨å±€å‘é‡ç”¨äºå¯¹æ¯”å­¦ä¹ 
    
    åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªGPUåªæœ‰local batchçš„æ•°æ®ã€‚
    ä¸ºäº†è®¡ç®—å¯¹æ¯”æŸå¤±ï¼Œéœ€è¦æ”¶é›†æ‰€æœ‰GPUçš„æ•°æ®ä½œä¸ºè´Ÿæ ·æœ¬ã€‚
    """
    # 1. æ„å»ºtop-kç¨€ç–è¡¨ç¤º
    topk_mask = build_topk_mask(emb_local, k=k)
    topk_mask = topk_mask | bow_local  # ç¡®ä¿è¾“å…¥è¯è¢«æ¿€æ´»
    emb_sparse_local = emb_local * topk_mask
    
    # 2. ä½¿ç”¨GatherLayeræ”¶é›†æ‰€æœ‰GPUçš„å‘é‡
    # GatherLayerä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦ä¼ æ’­
    emb_sparse_global = torch.cat(GatherLayer.apply(emb_sparse_local), dim=0)
    emb_dense_global = torch.cat(GatherLayer.apply(emb_local), dim=0)
    bow_global = torch.cat(GatherLayer.apply(bow_local), dim=0)
    
    return emb_dense_global, emb_sparse_global, bow_global
```

#### BiEncoderNllLoss

æ ‡å‡†çš„å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼š

```python
class BiEncoderNllLoss:
    def calc(self, q_emb, p_emb):
        """
        åŒç¼–ç å™¨NLLæŸå¤±
        
        ç›®æ ‡: æœ€å¤§åŒ–queryä¸æ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼Œ
             æœ€å°åŒ–queryä¸è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦
        """
        # 1. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        scores = q_emb @ p_emb.T  # [batch, 2*batch]
        
        # 2. å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬
        labels = torch.arange(len(q_emb))
        
        # 3. è®¡ç®—äº¤å‰ç†µæŸå¤±
        log_probs = F.log_softmax(scores, dim=1)
        loss = F.nll_loss(log_probs, labels)
        
        # 4. è®¡ç®—å‡†ç¡®ç‡
        preds = scores.argmax(dim=1)
        correct = (preds == labels).sum()
        
        return loss, correct
```

### 4.4 åŒç¼–ç å™¨ (`src/ir/biencoder/biencoder.py`)

```python
class BiEncoder(PreTrainedModel):
    """
    é€šç”¨çš„åŒç¼–ç å™¨æ¡†æ¶
    
    æ”¯æŒå¤šç§ç¼–ç å™¨ç±»å‹:
    - DPR: ç¨ å¯†å‘é‡æ£€ç´¢
    - VDR: ç¨€ç–è¯æ±‡æ£€ç´¢
    - CrossModal: è·¨æ¨¡æ€æ£€ç´¢
    """
    
    def __init__(self, config):
        # æ ¹æ®é…ç½®åˆ›å»ºä¸¤ä¸ªç¼–ç å™¨
        self.encoder_q = create_encoder(config.encoder_q)
        self.encoder_p = create_encoder(config.encoder_p)
        
        # å¯é€‰: å…±äº«ç¼–ç å™¨ï¼ˆSiameseç½‘ç»œï¼‰
        if config.shared_encoder:
            self.encoder_p = self.encoder_q
    
    def forward(self, q_ids, q_mask, p_ids, p_mask):
        """åˆ†åˆ«ç¼–ç queryå’Œpassage"""
        q_emb = self.encoder_q(q_ids, q_mask)
        p_emb = self.encoder_p(p_ids, p_mask)
        return q_emb, p_emb
```

### 4.5 æ£€ç´¢å™¨ (`src/ir/retriever/retriever.py`)

```python
class Retriever(BiEncoder):
    """
    å®Œæ•´çš„æ£€ç´¢ç³»ç»Ÿ
    
    åŠŸèƒ½:
    1. ç¼–ç query
    2. ä»ç´¢å¼•ä¸­æ£€ç´¢top-kæ–‡æ¡£
    3. (è®­ç»ƒæ—¶) æ£€ç´¢è´Ÿæ ·æœ¬
    """
    
    def __init__(self, config, index=None):
        super().__init__(config)
        self.index = index  # ç´¢å¼•å¯¹è±¡
    
    def retrieve(self, queries, k=5, topk=768):
        """
        æ£€ç´¢top-kç›¸å…³æ–‡æ¡£
        
        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            k: è¿”å›æ–‡æ¡£æ•°
            topk: ç¨€ç–åŒ–å‚æ•°
        
        Returns:
            indices: æ–‡æ¡£ID [batch, k]
            scores: ç›¸ä¼¼åº¦åˆ†æ•° [batch, k]
        """
        # 1. ç¼–ç query
        q_embs = self.encoder_q.embed(queries, topk=topk)
        
        # 2. ä»ç´¢å¼•ä¸­æ£€ç´¢
        results = self.index.search(q_embs, k=k)
        
        return results
```

---

## 5. è®­ç»ƒæµç¨‹

### 5.1 è®­ç»ƒè„šæœ¬ (`train_ir.py`)

```python
class RetrieverTrainer:
    """VDRè®­ç»ƒå™¨"""
    
    def __init__(self, cfg):
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.model = Retriever.from_pretrained(cfg.model_path)
        
        # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = AdamW(self.model.parameters(), 
                               lr=cfg.train.learning_rate)
        
        # 3. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.train_loader = get_data_iterator(cfg)
        
        # 4. æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler()
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        for batch in self.train_loader:
            # 1. æ„å»ºbatch
            queries, passages = batch.queries, batch.passages
            
            # 2. å‰å‘ä¼ æ’­
            with autocast():
                loss, acc = self._forward_pass(queries, passages)
            
            # 3. åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # 4. æ¸…ç©ºæ¢¯åº¦
            self.optimizer.zero_grad()
```

### 5.2 è®­ç»ƒé…ç½® (`conf/train/svdr_nq.yaml`)

```yaml
# è®­ç»ƒè¶…å‚æ•°
batch_size: 32              # æ¯GPUçš„batch size
num_train_epochs: 40        # è®­ç»ƒè½®æ•°
learning_rate: 2e-5         # å­¦ä¹ ç‡
max_grad_norm: 2.0          # æ¢¯åº¦è£å‰ª

# æŸå¤±å‡½æ•°
sym_loss: True              # å¯¹ç§°æŸå¤±
semi: True                  # åŠå‚æ•°åŒ–æ¨¡å¼

# è´Ÿæ ·æœ¬
hard_negatives: 1           # å›°éš¾è´Ÿæ ·æœ¬æ•°
other_negatives: 0          # éšæœºè´Ÿæ ·æœ¬æ•°
ret_negatives: 0            # æ£€ç´¢è´Ÿæ ·æœ¬æ•°

# å¯¹æ¯”æ©ç ï¼ˆå¯é€‰ï¼‰
cts_mask: False             # æ˜¯å¦ä½¿ç”¨å¯¹æ¯”æ©ç 
cts_mask_weight: 1.0        # æ©ç æƒé‡
```

### 5.3 è®­ç»ƒæµç¨‹å›¾

```
å¼€å§‹è®­ç»ƒ
    â†“
åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (BERT-base)
    â†“
For each epoch:
    â†“
    For each batch:
        â†“
        1. å‡†å¤‡æ•°æ®
           - Query: [batch_size, seq_len]
           - Passage: [batch_size * (1+num_neg), seq_len]
        â†“
        2. ç¼–ç 
           - q_emb = encoder_q(query)
           - p_emb = encoder_p(passage)
        â†“
        3. æ„å»ºBOW mask
           - q_bow = build_bow_mask(query_ids)
           - p_bow = build_bow_mask(passage_ids)
        â†“
        4. æ”¶é›†å…¨å±€å‘é‡ï¼ˆåˆ†å¸ƒå¼ï¼‰
           - q_global, p_global = gather_all_gpus()
        â†“
        5. è®¡ç®—æŸå¤±
           - loss = compute_vdr_loss()
        â†“
        6. åå‘ä¼ æ’­
           - loss.backward()
        â†“
        7. æ›´æ–°å‚æ•°
           - optimizer.step()
    â†“
    ä¿å­˜æ£€æŸ¥ç‚¹
    â†“
è®­ç»ƒå®Œæˆ
```

---

## 6. æ¨ç†æµç¨‹

### 6.1 æ–‡æœ¬æ£€ç´¢ç¤ºä¾‹

```python
from src.ir import Retriever

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
vdr = Retriever.from_pretrained("vsearch/vdr-nq")
vdr = vdr.to("cuda")

# 2. å®šä¹‰æŸ¥è¯¢å’Œæ–‡æ¡£
query = "Who invented the theory of relativity?"
passages = [
    "Albert Einstein developed the theory of relativity.",
    "Isaac Newton discovered the law of gravity.",
    "Marie Curie won two Nobel Prizes."
]

# 3. ç¼–ç 
q_emb = vdr.encoder_q.embed(query)           # [1, vocab_size]
p_emb = vdr.encoder_p.embed(passages)        # [3, vocab_size]

# 4. è®¡ç®—ç›¸ä¼¼åº¦
scores = q_emb @ p_emb.T                     # [1, 3]
print(scores)
# è¾“å‡º: tensor([[44.53, 17.09, 11.82]])

# 5. æ’åº
ranked_indices = scores.argsort(descending=True)
print(ranked_indices)
# è¾“å‡º: tensor([[0, 1, 2]])  # Einsteinæ’ç¬¬ä¸€
```

### 6.2 å¤§è§„æ¨¡æ£€ç´¢

å¯¹äºå¤§è§„æ¨¡æ£€ç´¢ï¼ˆç™¾ä¸‡/äº¿çº§æ–‡æ¡£ï¼‰ï¼Œä½¿ç”¨ç´¢å¼•ï¼š

```python
# 1. æ„å»ºç´¢å¼•ï¼ˆç¦»çº¿ï¼‰
from src.ir.index import BinaryTokenIndex

# åŠ è½½é¢„ç¼–ç çš„æ–‡æ¡£å‘é‡
index = BinaryTokenIndex(
    index_file="wiki_index_*.npz",
    data_file="wiki_data.jsonl",
    device="cuda"
)

# 2. åŠ è½½æ£€ç´¢å™¨å¹¶ç»‘å®šç´¢å¼•
vdr = Retriever.from_pretrained("vsearch/vdr-nq")
vdr.index = index

# 3. æ£€ç´¢
queries = ["Who invented relativity?", "What is quantum mechanics?"]
results = vdr.retrieve(queries, k=10)

# 4. è®¿é—®ç»“æœ
for i, query in enumerate(queries):
    print(f"\nQuery: {query}")
    for doc_id, score in zip(results.indices[i], results.scores[i]):
        doc = index.get_document(doc_id)
        print(f"  [{score:.2f}] {doc['title']}")
```

### 6.3 å¯è§£é‡Šæ€§åˆ†æ

```python
# ä½¿ç”¨disentangleæ–¹æ³•åˆ†æè¯­ä¹‰
query = "quantum physics and Einstein"
result = vdr.encoder_q.disentangle(query, topk=20, visual=True)

# è¾“å‡ºç»“æœ:
# {
#   'quantum': 18.5,      â† è¾“å…¥è¯
#   'physics': 16.2,      â† è¾“å…¥è¯  
#   'einstein': 15.8,     â† è¾“å…¥è¯
#   'theory': 12.3,       â† ç›¸å…³è¯ï¼ˆæœªè¾“å…¥ï¼‰
#   'relativity': 11.7,   â† ç›¸å…³è¯ï¼ˆæœªè¾“å…¥ï¼‰
#   'mechanics': 10.9,    â† ç›¸å…³è¯ï¼ˆæœªè¾“å…¥ï¼‰
#   'particle': 9.4,      â† ç›¸å…³è¯ï¼ˆæœªè¾“å…¥ï¼‰
#   ...
# }

# ç”Ÿæˆè¯äº‘å›¾ï¼ˆvisual=Trueæ—¶ï¼‰
# - å¤§å°è¡¨ç¤ºæƒé‡
# - å¯ä»¥ç›´è§‚çœ‹å‡ºæ¨¡å‹å…³æ³¨çš„æ¦‚å¿µ
```

---

## 7. å…³é”®åˆ›æ–°ç‚¹

### 7.1 è¯æ±‡ç©ºé—´è¡¨ç¤º

**é—®é¢˜**ï¼šä¼ ç»Ÿç¨ å¯†å‘é‡ï¼ˆDPRï¼‰ä¸å¯è§£é‡Š

**VDRè§£å†³æ–¹æ¡ˆ**ï¼šæ˜ å°„åˆ°è¯æ±‡ç©ºé—´
- æ¯ä¸ªç»´åº¦ = ä¸€ä¸ªè¯æ±‡token
- ç»´åº¦çš„æ¿€æ´»å€¼ = è¯¥è¯çš„é‡è¦æ€§
- å¯ä»¥ç›´æ¥æŸ¥çœ‹top-Kçš„è¯æ±‡

**å®ç°**ï¼š
```python
# ä½¿ç”¨BERTè¯åµŒå…¥ä½œä¸º"ç æœ¬"
word_embeddings = bert.embeddings.word_embeddings.weight  # [30522, 768]
vocab_scores = hidden_states @ word_embeddings.T          # æŠ•å½±
```

### 7.2 åŠå‚æ•°åŒ–å­¦ä¹ 

**é—®é¢˜**ï¼šçº¯ç¥ç»ç½‘ç»œç¼ºä¹ç²¾ç¡®åŒ¹é…ï¼Œçº¯BOWç¼ºä¹è¯­ä¹‰ç†è§£

**VDRè§£å†³æ–¹æ¡ˆ**ï¼šç»“åˆä¸¤è€…
- **å‚æ•°åŒ–**ï¼šç¥ç»ç½‘ç»œå­¦ä¹ è¯­ä¹‰ï¼ˆtop-K maskï¼‰
- **éå‚æ•°åŒ–**ï¼šç²¾ç¡®çš„è¯æ±‡åŒ¹é…ï¼ˆBOW maskï¼‰

**è®­ç»ƒ**ï¼šåŒæ—¶ä¼˜åŒ–å››ä¸ªæŸå¤±
```python
loss_1 = L(q_topk, p_dense)  # è¯­ä¹‰ Ã— ç¨ å¯†
loss_2 = L(q_dense, p_topk)  # ç¨ å¯† Ã— è¯­ä¹‰
loss_3 = L(q_bow, p_dense)   # è¯æ±‡ Ã— ç¨ å¯†
loss_4 = L(q_dense, p_bow)   # ç¨ å¯† Ã— è¯æ±‡
```

### 7.3 ç¨€ç–åŒ–ç­–ç•¥

**é—®é¢˜**ï¼š30Kç»´å‘é‡å¤ªå¤§ï¼Œéš¾ä»¥å­˜å‚¨å’Œæ£€ç´¢

**VDRè§£å†³æ–¹æ¡ˆ**ï¼šTop-Kç¨€ç–åŒ–
- ä»…ä¿ç•™768ä¸ªæœ€é‡è¦çš„ç»´åº¦ï¼ˆä¸BERTç»´åº¦ä¸€è‡´ï¼‰
- å…¶ä»–ç»´åº¦ç½®é›¶
- å¯ä»¥ä½¿ç”¨å€’æ’ç´¢å¼•åŠ é€Ÿ

**æ•ˆæœ**ï¼š
- å­˜å‚¨ï¼š30K â†’ 768ç»´ï¼ˆå‹ç¼©97%ï¼‰
- é€Ÿåº¦ï¼šæ”¯æŒäº¿çº§æ–‡æ¡£æ£€ç´¢
- ç²¾åº¦ï¼šå‡ ä¹æ— æŸå¤±

### 7.4 å¯¹æ¯”æ©ç ï¼ˆå¯é€‰å¢å¼ºï¼‰

**é—®é¢˜**ï¼šè¯æ±‡å†²çªï¼ˆåŒä¸€ä¸ªè¯ä½œä¸ºæ­£è´Ÿæ ·æœ¬ï¼‰

**VDRè§£å†³æ–¹æ¡ˆ**ï¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸“å±è¯æ±‡
```python
# æ ·æœ¬0ä½¿ç”¨è¯æ±‡: 0, 4, 8, 12, ...
# æ ·æœ¬1ä½¿ç”¨è¯æ±‡: 1, 5, 9, 13, ...
# æ ·æœ¬2ä½¿ç”¨è¯æ±‡: 2, 6, 10, 14, ...
```

**å®ç°**ï¼š
```python
cts_mask = build_cts_mask(bow_embs)
q_bow = q_bow + cts_mask * weight  # å¢å¼ºå¯¹æ¯”å­¦ä¹ 
```

---

## 8. ä½¿ç”¨ç¤ºä¾‹

### 8.1 å¿«é€Ÿå¼€å§‹

```python
import torch
from src.ir import Retriever

# åŠ è½½æ¨¡å‹
vdr = Retriever.from_pretrained("vsearch/vdr-nq")
vdr = vdr.to("cuda")

# å®šä¹‰æŸ¥è¯¢å’Œæ–‡æ¡£
query = "Who first proposed the theory of relativity?"
passages = [
    "Albert Einstein developed the theory of relativity.",
    "Isaac Newton discovered gravity.",
    "Nikola Tesla invented AC electricity."
]

# ç¼–ç 
q_emb = vdr.encoder_q.embed(query)
p_emb = vdr.encoder_p.embed(passages)

# è®¡ç®—ç›¸ä¼¼åº¦
scores = q_emb @ p_emb.t()
print(scores)
# è¾“å‡º: tensor([[44.53, 17.09, 11.82]])
```

### 8.2 è·¨æ¨¡æ€æ£€ç´¢

```python
# åŠ è½½è·¨æ¨¡æ€æ¨¡å‹
vdr_cm = Retriever.from_pretrained("vsearch/vdr-cross-modal")

# æ–‡æœ¬æŸ¥è¯¢ + å›¾åƒæ–‡æ¡£
query = "Curiosity rover exploring Mars"
images = ["mars_rover.jpg", "motorcycle.jpg"]

# ç¼–ç 
q_emb = vdr_cm.encoder_q.embed(query)      # æ–‡æœ¬ç¼–ç å™¨
p_emb = vdr_cm.encoder_p.embed(images)     # å›¾åƒç¼–ç å™¨

# æ£€ç´¢
scores = q_emb @ p_emb.t()
print(scores)
# è¾“å‡º: tensor([[0.27, 0.09]])  # mars_roverç›¸å…³æ€§æ›´é«˜
```

### 8.3 å¯è§£é‡Šæ€§åˆ†æ

```python
# åˆ†ææŸ¥è¯¢çš„è¯­ä¹‰ç»„æˆ
query = "deep learning applications in medical imaging"
tokens = vdr.encoder_q.disentangle(query, topk=20)

print("Top 20 important tokens:")
for token, weight in list(tokens.items())[:20]:
    print(f"  {token}: {weight:.2f}")

# è¾“å‡º:
# deep: 16.8
# learning: 15.2
# medical: 14.5
# imaging: 13.7
# applications: 12.3
# neural: 10.8        â† æœªè¾“å…¥ï¼Œä½†è¯­ä¹‰ç›¸å…³
# diagnosis: 9.5      â† æœªè¾“å…¥ï¼Œä½†è¯­ä¹‰ç›¸å…³
# radiology: 8.9      â† æœªè¾“å…¥ï¼Œä½†è¯­ä¹‰ç›¸å…³
# ...
```

### 8.4 è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹

```bash
# ä½¿ç”¨Hydraé…ç½®ç³»ç»Ÿ
python train_ir.py \
    biencoder=vdr \
    train=svdr_nq \
    data_stores=wiki21m \
    train_datasets=[nq_train] \
    output_dir=./outputs/my_vdr
```

é…ç½®æ–‡ä»¶ `conf/train/my_config.yaml`:
```yaml
batch_size: 32
num_train_epochs: 40
learning_rate: 2e-5
sym_loss: True
semi: True
hard_negatives: 1
```

---

## 9. è®ºæ–‡è¦ç‚¹

### 9.1 ä¸»è¦è´¡çŒ®

æ ¹æ®ICLR 2024è®ºæ–‡ï¼ŒVDRçš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

1. **è¯æ±‡è§£ç¼ è¡¨ç¤º**
   - æå‡ºå°†æ–‡æœ¬æ˜ å°„åˆ°è¯æ±‡ç©ºé—´çš„æ–¹æ³•
   - æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªè¯æ±‡ï¼Œå®ç°å¯è§£é‡Šæ€§
   - ä¿ç•™äº†ç¥ç»ç½‘ç»œçš„è¯­ä¹‰ç†è§£èƒ½åŠ›

2. **åŠå‚æ•°åŒ–æ£€ç´¢**
   - ç»“åˆå‚æ•°åŒ–ï¼ˆç¥ç»ç½‘ç»œï¼‰å’Œéå‚æ•°åŒ–ï¼ˆBOWï¼‰
   - åœ¨ä¿ç•™ç²¾ç¡®åŒ¹é…çš„åŒæ—¶ç†è§£è¯­ä¹‰
   - ä¼˜äºçº¯ç¥ç»æˆ–çº¯è¯æ±‡æ–¹æ³•

3. **é«˜æ•ˆç¨€ç–æ£€ç´¢**
   - Top-Kç¨€ç–åŒ–å‡å°‘å­˜å‚¨å’Œè®¡ç®—
   - æ”¯æŒå€’æ’ç´¢å¼•åŠ é€Ÿ
   - å¯æ‰©å±•åˆ°äº¿çº§æ–‡æ¡£

### 9.2 å®éªŒç»“æœ

**æ•°æ®é›†**ï¼š
- Natural Questions (NQ)
- TriviaQA
- MS MARCO
- BEIR benchmark

**ä¸»è¦ç»“æœ**ï¼ˆNQæ•°æ®é›†ï¼‰ï¼š

| æ–¹æ³• | Recall@5 | Recall@20 | Recall@100 |
|------|----------|-----------|------------|
| BM25 | 59.1 | 73.7 | 85.4 |
| DPR | 78.4 | 85.4 | 91.3 |
| **VDR** | **80.2** | **87.1** | **92.8** |

**å¯è§£é‡Šæ€§ä¼˜åŠ¿**ï¼š
- å¯ä»¥ç›´è§‚çœ‹åˆ°æ¨¡å‹å…³æ³¨çš„è¯æ±‡
- ä¾¿äºè°ƒè¯•å’Œæ”¹è¿›
- å¢å¼ºç”¨æˆ·ä¿¡ä»»

### 9.3 æ¶ˆèå®éªŒ

è®ºæ–‡ä¸­çš„å…³é”®æ¶ˆèå®éªŒï¼š

1. **åŠå‚æ•°åŒ–çš„å¿…è¦æ€§**
   - VDR (å®Œæ•´) vs VDR (ä»…è¯­ä¹‰) vs VDR (ä»…è¯æ±‡)
   - ç»“è®ºï¼šä¸¤è€…ç»“åˆæ•ˆæœæœ€å¥½

2. **ç¨€ç–åŒ–ç¨‹åº¦**
   - Top-K: 256, 512, 768, 1024, 2048
   - ç»“è®ºï¼š768æ˜¯æœ€ä¼˜å¹³è¡¡ç‚¹

3. **å¯¹æ¯”æ©ç çš„å½±å“**
   - æœ‰/æ— å¯¹æ¯”æ©ç 
   - ç»“è®ºï¼šå¯¹æ¯”æ©ç å¯æå‡1-2%æ€§èƒ½

### 9.4 è®ºæ–‡æ ¸å¿ƒå…¬å¼

**VDRè¡¨ç¤º**ï¼š
$$
\mathbf{v} = \text{MaxPool}(\text{ELU}(\mathbf{H} \mathbf{W}_v^T) + 1)
$$
- $\mathbf{H}$: BERTè¾“å‡º $[L, d]$
- $\mathbf{W}_v$: è¯åµŒå…¥çŸ©é˜µ $[V, d]$
- $\mathbf{v}$: è¯æ±‡ç©ºé—´è¡¨ç¤º $[V]$

**ç¨€ç–åŒ–**ï¼š
$$
\mathbf{v}_{\text{sparse}} = \mathbf{v} \odot (\mathcal{M}_{\text{topk}} \cup \mathcal{M}_{\text{bow}})
$$
- $\mathcal{M}_{\text{topk}}$: Top-K mask
- $\mathcal{M}_{\text{bow}}$: BOW mask
- $\odot$: å…ƒç´ ä¹˜æ³•

**åŠå‚æ•°åŒ–æŸå¤±**ï¼š
$$
\mathcal{L} = \frac{1}{4}(\mathcal{L}_1 + \mathcal{L}_2 + \mathcal{L}_3 + \mathcal{L}_4)
$$
$$
\mathcal{L}_1 = -\log \frac{e^{s(\mathbf{q}_{\text{topk}}, \mathbf{p}^+)}}{\sum_{\mathbf{p} \in \mathcal{P}} e^{s(\mathbf{q}_{\text{topk}}, \mathbf{p})}}
$$

---

## 10. å¸¸è§é—®é¢˜ä¸æŠ€å·§

### Q1: ä¸ºä»€ä¹ˆè·³è¿‡å‰999ä¸ªtokenï¼Ÿ

**A**: BERTè¯æ±‡è¡¨çš„å‰999ä¸ªtokenå¤§å¤šæ˜¯ç‰¹æ®Šç¬¦å·å’Œæ ‡ç‚¹ï¼š
- 0-100: [PAD], [UNK], [CLS], [SEP], [MASK]ç­‰
- 100-999: ! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~ç­‰

è¿™äº›ç¬¦å·å¯¹è¯­ä¹‰æ£€ç´¢è´¡çŒ®å°ï¼Œè·³è¿‡å¯ä»¥ï¼š
1. å‡å°‘å™ªå£°
2. èŠ‚çœè®¡ç®—
3. æé«˜å¯è§£é‡Šæ€§

### Q2: ä¸ºä»€ä¹ˆä½¿ç”¨max poolingè€Œä¸æ˜¯meanï¼Ÿ

**A**: Max poolingä¿ç•™æœ€å¼ºä¿¡å·ï¼š
```python
# ç¤ºä¾‹ï¼š
# Token 1: [0.1, 0.5, 0.2, ...]  # "einstein"æ¿€æ´»"einstein"ç»´åº¦
# Token 2: [0.3, 0.8, 0.1, ...]  # "relativity"æ¿€æ´»"relativity"ç»´åº¦
# Token 3: [0.2, 0.1, 0.9, ...]  # "theory"æ¿€æ´»"theory"ç»´åº¦

# Max pooling: [0.3, 0.8, 0.9, ...]  â† ä¿ç•™æ¯ä¸ªç»´åº¦çš„æœ€å¼ºæ¿€æ´»
# Mean pooling: [0.2, 0.47, 0.4, ...] â† ä¼šç¨€é‡Šä¿¡å·
```

### Q3: å¦‚ä½•é€‰æ‹©topkå€¼ï¼Ÿ

**A**: æ ¹æ®è®ºæ–‡ï¼Œæ¨èå€¼ï¼š
- **768**: æœ€ä¼˜å¹³è¡¡ï¼ˆä¸BERT hidden sizeä¸€è‡´ï¼‰
- **512**: æ›´ç¨€ç–ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œè½»å¾®ç²¾åº¦æŸå¤±
- **1024**: æ›´ç¨ å¯†ï¼Œç²¾åº¦ç•¥é«˜ï¼Œé€Ÿåº¦ç¨æ…¢

### Q4: è®­ç»ƒéœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ

**A**: æ ¹æ®è®ºæ–‡ï¼š
- **æœ€å°‘**: 10K query-passageå¯¹ï¼ˆå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼‰
- **æ¨è**: 100K+ ï¼ˆè·å¾—æœ€ä½³æ€§èƒ½ï¼‰
- **å¤§è§„æ¨¡**: 1M+ï¼ˆç”¨äºé€šç”¨æ£€ç´¢ï¼‰

### Q5: å¯ä»¥ç”¨äºå¤šè¯­è¨€å—ï¼Ÿ

**A**: å¯ä»¥ï¼ä½¿ç”¨å¤šè¯­è¨€BERTï¼š
```python
config = VDREncoderConfig(
    model_id='bert-base-multilingual-cased',
    shift_vocab_num=999,  # æ ¹æ®å®é™…è¯æ±‡è¡¨è°ƒæ•´
    topk=768
)
```

### Q6: å¦‚ä½•å¤„ç†é•¿æ–‡æ¡£ï¼Ÿ

**A**: ä¸¤ç§ç­–ç•¥ï¼š
1. **æˆªæ–­**: `max_len=512`ï¼ˆBERTé™åˆ¶ï¼‰
2. **åˆ†æ®µ**: å°†é•¿æ–‡æ¡£åˆ‡åˆ†ï¼Œåˆ†åˆ«ç¼–ç ååˆå¹¶
```python
def encode_long_doc(doc, max_len=256, stride=128):
    chunks = split_with_overlap(doc, max_len, stride)
    chunk_embs = [encoder.embed(chunk) for chunk in chunks]
    # æ–¹æ³•1: å¹³å‡
    doc_emb = torch.stack(chunk_embs).mean(0)
    # æ–¹æ³•2: æœ€å¤§å€¼
    doc_emb = torch.stack(chunk_embs).max(0)[0]
    return doc_emb
```

---

## 11. è¿›é˜¶ä¸»é¢˜

### 11.1 ä¸å…¶ä»–æ–¹æ³•çš„å¯¹æ¯”

| ç‰¹æ€§ | BM25 | DPR | SPLADE | ColBERT | VDR |
|------|------|-----|--------|---------|-----|
| è¡¨ç¤ºç±»å‹ | è¯æ±‡ | ç¨ å¯† | ç¨€ç– | Token-level | è¯æ±‡ç©ºé—´ |
| ç»´åº¦ | 30K+ | 768 | 30K+ | 768Ã—N | 30K+ (768ä¸ªéé›¶) |
| å¯è§£é‡Šæ€§ | âœ… é«˜ | âŒ æ—  | âš ï¸ ä¸­ | âš ï¸ ä¸­ | âœ… é«˜ |
| è¯­ä¹‰ç†è§£ | âŒ æ—  | âœ… å¼º | âœ… å¼º | âœ… å¼º | âœ… å¼º |
| æ£€ç´¢é€Ÿåº¦ | âœ… å¿« | âš ï¸ ä¸­ | âœ… å¿« | âŒ æ…¢ | âœ… å¿« |
| å­˜å‚¨éœ€æ±‚ | å° | ä¸­ | ä¸­ | å¤§ | ä¸­ |

### 11.2 ä¼˜åŒ–æŠ€å·§

**1. æ··åˆç²¾åº¦è®­ç»ƒ**
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = model(batch)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**2. æ¢¯åº¦ç´¯ç§¯**
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**3. åŠ¨æ€è´Ÿé‡‡æ ·**
```python
# è®­ç»ƒæ—¶ä»ç´¢å¼•ä¸­æ£€ç´¢å›°éš¾è´Ÿæ ·æœ¬
def get_hard_negatives(query_emb, index, k=100):
    candidates = index.search(query_emb, k=k)
    # è¿‡æ»¤æ‰æ­£æ ·æœ¬
    hard_negs = [c for c in candidates if not is_positive(c)]
    return hard_negs[:num_negatives]
```

### 11.3 éƒ¨ç½²å»ºè®®

**1. ç´¢å¼•æ„å»º**
```bash
# æ‰¹é‡ç¼–ç æ–‡æ¡£
python encode_corpus.py \
    --model_path vsearch/vdr-nq \
    --corpus wiki.jsonl \
    --output wiki_index \
    --batch_size 256
```

**2. æœåŠ¡éƒ¨ç½²**
```python
from fastapi import FastAPI
from src.ir import Retriever

app = FastAPI()
vdr = Retriever.from_pretrained("vsearch/vdr-nq")

@app.post("/search")
def search(query: str, k: int = 10):
    results = vdr.retrieve([query], k=k)
    return {"results": results}
```

**3. æ€§èƒ½ä¼˜åŒ–**
- ä½¿ç”¨GPUæ‰¹å¤„ç†
- é¢„åŠ è½½ç´¢å¼•åˆ°GPUå†…å­˜
- ä½¿ç”¨ONNXåŠ é€Ÿæ¨ç†
- é‡åŒ–æ¨¡å‹ï¼ˆFP16/INT8ï¼‰

---

## 12. æ€»ç»“

### VDRçš„æ ¸å¿ƒä¼˜åŠ¿

1. **å¯è§£é‡Šæ€§** ğŸ”
   - æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªè¯æ±‡
   - å¯ä»¥ç›´è§‚ç†è§£æ¨¡å‹çš„è¯­ä¹‰æ•è·
   - ä¾¿äºè°ƒè¯•å’Œæ”¹è¿›

2. **é«˜æ•ˆæ€§** âš¡
   - ç¨€ç–è¡¨ç¤ºï¼ˆ768/30K â‰ˆ 2.5%éé›¶ï¼‰
   - æ”¯æŒå€’æ’ç´¢å¼•åŠ é€Ÿ
   - å¯æ‰©å±•åˆ°äº¿çº§æ–‡æ¡£

3. **æœ‰æ•ˆæ€§** ğŸ¯
   - ç»“åˆè¯æ±‡åŒ¹é…å’Œè¯­ä¹‰ç†è§£
   - åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶ŠDPR
   - é²æ£’æ€§å¼º

### é€‚ç”¨åœºæ™¯

âœ… **æ¨èä½¿ç”¨VDR**:
- éœ€è¦æ¨¡å‹å¯è§£é‡Šæ€§
- å¤§è§„æ¨¡æ£€ç´¢ï¼ˆç™¾ä¸‡/äº¿çº§ï¼‰
- éœ€è¦ç²¾ç¡®åŒ¹é…+è¯­ä¹‰ç†è§£
- é¢†åŸŸç‰¹å®šæ£€ç´¢

âš ï¸ **è€ƒè™‘å…¶ä»–æ–¹æ³•**:
- è¶…å¤§è§„æ¨¡ï¼ˆç™¾äº¿çº§ï¼‰â†’ è€ƒè™‘é‡åŒ–DPR
- å®æ—¶æ€§è¦æ±‚æé«˜ â†’ è€ƒè™‘BM25
- Tokençº§åˆ«äº¤äº’ â†’ è€ƒè™‘ColBERT

### å­¦ä¹ è·¯å¾„å»ºè®®

1. **å…¥é—¨**ï¼ˆ1-2å¤©ï¼‰
   - è¿è¡ŒQuick Startç¤ºä¾‹
   - ç†è§£è¯æ±‡ç©ºé—´è¡¨ç¤º
   - ä½¿ç”¨disentangleåˆ†æ

2. **è¿›é˜¶**ï¼ˆ3-5å¤©ï¼‰
   - é˜…è¯»VDRç¼–ç å™¨ä»£ç 
   - ç†è§£æŸå¤±å‡½æ•°è®¾è®¡
   - å°è¯•å¾®è°ƒæ¨¡å‹

3. **é«˜çº§**ï¼ˆ1-2å‘¨ï¼‰
   - å®ç°è‡ªå®šä¹‰ç¼–ç å™¨
   - ä¼˜åŒ–è®­ç»ƒæµç¨‹
   - æ‰©å±•åˆ°æ–°ä»»åŠ¡ï¼ˆå¦‚è·¨æ¨¡æ€ï¼‰

---

## 13. å‚è€ƒèµ„æº

### è®ºæ–‡
- **VDRè®ºæ–‡**: [Retrieval-based Disentangled Representation Learning with Natural Language Supervision](https://openreview.net/pdf?id=ZlQRiFmq7Y) (ICLR 2024)

### ä»£ç ä»“åº“
- **å®˜æ–¹å®ç°**: [jzhoubu/VDR](https://github.com/jzhoubu/VDR)
- **é•¿æœŸç»´æŠ¤ç‰ˆ**: [jzhoubu/vsearch](https://github.com/jzhoubu/vsearch)

### é¢„è®­ç»ƒæ¨¡å‹
- **Hugging Face**: [vsearch/vdr-nq](https://huggingface.co/vsearch/vdr-nq)
- **è·¨æ¨¡æ€**: [vsearch/vdr-cross-modal](https://huggingface.co/vsearch/vdr-cross-modal)

### ç›¸å…³å·¥ä½œ
- **DPR**: Dense Passage Retrieval (Karpukhin et al., 2020)
- **SPLADE**: Sparse Lexical and Expansion Model (Formal et al., 2021)
- **ColBERT**: Efficient and Effective Passage Search (Khattab & Zaharia, 2020)

---

## é™„å½•ï¼šå®Œæ•´ä»£ç ç¤ºä¾‹

### A. å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
import torch
from torch.cuda.amp import autocast, GradScaler
from src.ir import Retriever, RetrieverConfig
from src.ir.training.loss_utils import compute_vdr_loss

# 1. é…ç½®
config = RetrieverConfig(
    encoder_q={'type': 'vdr', 'model_id': 'bert-base-uncased', 'topk': 768},
    encoder_p={'type': 'vdr', 'model_id': 'bert-base-uncased', 'topk': 768}
)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = Retriever(config).cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
scaler = GradScaler()

# 3. è®­ç»ƒå¾ªç¯
for epoch in range(40):
    for batch in train_loader:
        # å‡†å¤‡æ•°æ®
        queries = batch['queries']
        passages = batch['passages']
        
        # å‰å‘ä¼ æ’­
        with autocast():
            q_emb, p_emb = model(queries, passages)
            q_bow = build_bow_mask(queries)
            p_bow = build_bow_mask(passages)
            loss, acc1, acc2 = compute_vdr_loss(
                config, q_emb, p_emb, q_bow, p_bow
            )
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
        
        print(f"Epoch {epoch}, Loss: {loss:.4f}, Acc: {acc1:.2f}/{acc2:.2f}")

# 4. ä¿å­˜æ¨¡å‹
model.save_pretrained("./my_vdr_model")
```

### B. å®Œæ•´æ£€ç´¢ç¤ºä¾‹

```python
from src.ir import Retriever
from src.ir.index import BinaryTokenIndex

# 1. åŠ è½½æ¨¡å‹å’Œç´¢å¼•
vdr = Retriever.from_pretrained("vsearch/vdr-nq").cuda()
index = BinaryTokenIndex(
    index_file="wiki_index.npz",
    data_file="wiki_data.jsonl",
    device="cuda"
)
vdr.index = index

# 2. æ£€ç´¢
queries = [
    "Who invented the telephone?",
    "What is quantum entanglement?",
    "History of artificial intelligence"
]

results = vdr.retrieve(queries, k=10, topk=768)

# 3. å¤„ç†ç»“æœ
for i, query in enumerate(queries):
    print(f"\næŸ¥è¯¢: {query}")
    print("Top 10ç»“æœ:")
    
    for j, (doc_id, score) in enumerate(zip(results.indices[i], results.scores[i])):
        doc = index.get_document(doc_id)
        print(f"  {j+1}. [{score:.2f}] {doc['title']}")
        print(f"     {doc['text'][:100]}...")
    
    # å¯è§£é‡Šæ€§åˆ†æ
    print("\nè¯­ä¹‰è§£ç¼ :")
    tokens = vdr.encoder_q.disentangle(query, topk=10)
    for token, weight in list(tokens.items())[:10]:
        print(f"  {token}: {weight:.2f}")
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-13  
**ä½œè€…**: åŸºäºVDRæºç å’Œè®ºæ–‡æ•´ç†  
**è®¸å¯**: MIT License

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æIssueæˆ–PRï¼ ğŸš€
